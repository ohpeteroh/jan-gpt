import os
import tempfile
from pathlib import Path
import streamlit as st
import pandas as pd
import subprocess
from pptx import Presentation
import pyxlsb
import pytesseract
from PIL import Image
from duckduckgo_search import DDGS

from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI

# ğŸ” API Key
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]

embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
llm = ChatOpenAI(model="gpt-4", temperature=0.2, openai_api_key=OPENAI_API_KEY)
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
DB_PATH = "faiss_index"

def load_and_split_file(tmp_path, suffix):
    docs = []
    if suffix == ".txt":
        with open(tmp_path, encoding="utf-8") as f:
            docs = [Document(page_content=f.read())]
    elif suffix == ".pdf":
        from langchain_community.document_loaders import PyPDFLoader
        docs = PyPDFLoader(tmp_path).load()
    elif suffix == ".docx":
        from langchain_community.document_loaders import UnstructuredWordDocumentLoader
        docs = UnstructuredWordDocumentLoader(tmp_path).load()
    elif suffix == ".pptx":
        prs = Presentation(tmp_path)
        text = "\n".join([shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, "text")])
        docs = [Document(page_content=text)]
    elif suffix == ".hwp":
        result = subprocess.run(['hwp5txt', tmp_path], stdout=subprocess.PIPE, encoding='utf-8')
        docs = [Document(page_content=result.stdout)]
    elif suffix in [".xlsx", ".xlsm"]:
        df = pd.read_excel(tmp_path, engine='openpyxl')
        docs = [Document(page_content=df.to_string())]
    elif suffix == ".xlsb":
        with pyxlsb.open_workbook(tmp_path) as wb:
            sheet = wb.get_sheet(1)
            data = "\n".join(["\t".join([str(cell.v) for cell in row]) for row in sheet.rows()])
        docs = [Document(page_content=data)]
    elif suffix in [".csv"]:
        df = pd.read_csv(tmp_path)
        docs = [Document(page_content=df.to_string())]
    elif suffix in [".png", ".jpg", ".jpeg"]:
        text = pytesseract.image_to_string(Image.open(tmp_path), lang='eng+kor')
        docs = [Document(page_content=text)]

    if not docs:
        return False

    chunks = splitter.split_documents(docs)
    if not os.path.exists(DB_PATH):
        db = FAISS.from_documents(chunks, embedding)
    else:
        db = FAISS.load_local(DB_PATH, embedding)
        db.add_documents(chunks)
    db.save_local(DB_PATH)
    return True

st.set_page_config(page_title="Jan GPT", layout="wide")
st.title("ğŸ“‚ Jan GPT - ë¬¸ì„œ + ì´ë¯¸ì§€ + ì›¹ ê²€ìƒ‰ ê¸°ë°˜ ë¦¬ì„œì¹˜ GPT")

uploaded_file = st.file_uploader("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ (.txt, .pdf, .docx, .pptx, .hwp, .xlsx, .xlsm, .xlsb, .csv, .png, .jpg)",
                                 type=["txt", "pdf", "docx", "pptx", "hwp", "xlsx", "xlsm", "xlsb", "csv", "png", "jpg", "jpeg"])

if uploaded_file:
    suffix = Path(uploaded_file.name).suffix.lower()
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        tmp.write(uploaded_file.getvalue())
        tmp_path = tmp.name
    if load_and_split_file(tmp_path, suffix):
        st.success("âœ… íŒŒì¼ì´ ìë™ìœ¼ë¡œ ë²¡í„°í™”ë˜ì–´ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.")

query = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
use_web = st.checkbox("ğŸŒ ì›¹ ê²€ìƒ‰ë„ í¬í•¨í• ê¹Œìš”? (DuckDuckGo ê¸°ë°˜)", value=False)
search_mode = st.radio("ê²€ìƒ‰ ëª¨ë“œ", ["ì¼ë°˜ ê²€ìƒ‰", "ì‹¬ì¸µ ë¦¬ì„œì¹˜"], horizontal=True)

if query:
    try:
        if os.path.exists(DB_PATH):
            db = FAISS.load_local(DB_PATH, embedding)
            docs = db.similarity_search(query, k=5)
            doc_context = "\n\n".join([doc.page_content for doc in docs])
        else:
            doc_context = "(ë¬¸ì„œ ì—†ìŒ)"

        # DuckDuckGo ê²€ìƒ‰
        web_results = ""
        if use_web:
            try:
                ddgs = DDGS()
                results = ddgs.text(query, max_results=5)
                web_results = "\n".join([r["body"] for r in results])
            except Exception as e:
                web_results = f"(ì›¹ ê²€ìƒ‰ ì‹¤íŒ¨: {e})"

        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"[ë¬¸ì„œ ê¸°ë°˜ ì •ë³´]\n{doc_context}\n"
        if use_web:
            prompt += f"\n[ì›¹ ê²€ìƒ‰ ì •ë³´]\n{web_results}\n"

        if search_mode == "ì‹¬ì¸µ ë¦¬ì„œì¹˜":
            prompt += f"\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{query}'ì— ëŒ€í•´ ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•œ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:\n1. í•µì‹¬ ìš”ì•½\n2. ì£¼ìš” ê·¼ê±° ë° ë°°ê²½ ì •ë³´\n3. ì „ëµì  ì‹œì‚¬ì  ë° ì œì–¸"
        else:
            prompt += f"\nìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{query}'ì— ë‹µë³€í•´ ì£¼ì„¸ìš”."

        response = llm.invoke(prompt)
        st.markdown("### ğŸ’¬ GPT ì‘ë‹µ")
        st.write(response.content)

    except Exception as e:
        st.error(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
