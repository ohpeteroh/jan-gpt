import os
import tempfile
from pathlib import Path
import streamlit as st
import pandas as pd
import subprocess
from pptx import Presentation
import pyxlsb

from langchain.docstore.document import Document
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_community.utilities import SerpAPIWrapper
from langchain_community.tools import Tool
from langchain_core.tools import Tool as BaseTool

# ğŸ” Secrets ë¶ˆëŸ¬ì˜¤ê¸°
OPENAI_API_KEY = st.secrets["OPENAI_API_KEY"]
SERPAPI_API_KEY = st.secrets["SERPAPI_API_KEY"]

# LangChain êµ¬ì„± ìš”ì†Œ ì´ˆê¸°í™”
embedding = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)
llm = ChatOpenAI(model="gpt-4", temperature=0.2, openai_api_key=OPENAI_API_KEY)
search = SerpAPIWrapper(serpapi_api_key=SERPAPI_API_KEY)
search_tool: BaseTool = Tool(
    name="Google Search",
    description="Search the internet using Google via SerpAPI",
    func=search.run,
)
splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)
DB_PATH = "faiss_index"

# íŒŒì¼ ë¡œë”© ë° ë²¡í„°í™” í•¨ìˆ˜
def load_and_split_file(tmp_path, suffix):
    docs = []
    if suffix == ".txt":
        with open(tmp_path, encoding="utf-8") as f:
            text = f.read()
        docs = [Document(page_content=text)]
    elif suffix == ".pdf":
        from langchain_community.document_loaders import PyPDFLoader
        docs = PyPDFLoader(tmp_path).load()
    elif suffix == ".docx":
        from langchain_community.document_loaders import UnstructuredWordDocumentLoader
        docs = UnstructuredWordDocumentLoader(tmp_path).load()
    elif suffix == ".pptx":
        prs = Presentation(tmp_path)
        text = "\n".join([shape.text for slide in prs.slides for shape in slide.shapes if hasattr(shape, "text")])
        docs = [Document(page_content=text)]
    elif suffix == ".hwp":
        result = subprocess.run(['hwp5txt', tmp_path], stdout=subprocess.PIPE, encoding='utf-8')
        docs = [Document(page_content=result.stdout)]
    elif suffix in [".xlsx", ".xlsm"]:
        df = pd.read_excel(tmp_path, engine='openpyxl')
        docs = [Document(page_content=df.to_string())]
    elif suffix == ".xlsb":
        with pyxlsb.open_workbook(tmp_path) as wb:
            sheet = wb.get_sheet(1)
            data = "\n".join(["\t".join([str(cell.v) for cell in row]) for row in sheet.rows()])
        docs = [Document(page_content=data)]
    elif suffix == ".csv":
        df = pd.read_csv(tmp_path)
        docs = [Document(page_content=df.to_string())]

    chunks = splitter.split_documents(docs)
    if not os.path.exists(DB_PATH):
        db = FAISS.from_documents(chunks, embedding)
    else:
        db = FAISS.load_local(DB_PATH, embedding)
        db.add_documents(chunks)
    db.save_local(DB_PATH)
    return True

# Streamlit UI
st.set_page_config(page_title="Jan GPT", layout="wide")
st.title("ğŸ“‚ Jan GPT - íŒŒì¼ ê¸°ë°˜ ê²€ìƒ‰ & ë¦¬ì„œì¹˜ AI")

uploaded_file = st.file_uploader("ğŸ“¤ íŒŒì¼ ì—…ë¡œë“œ (.txt, .pdf, .docx, .pptx, .hwp, .xlsx, .xlsm, .xlsb, .csv)",
                                 type=["txt", "pdf", "docx", "pptx", "hwp", "xlsx", "xlsm", "xlsb", "csv"])

if uploaded_file:
    suffix = Path(uploaded_file.name).suffix.lower()
    with tempfile.NamedTemporaryFile(delete=False, suffix=suffix) as tmp:
        tmp.write(uploaded_file.getvalue())
        tmp_path = tmp.name
    success = load_and_split_file(tmp_path, suffix)
    if success:
        st.success("âœ… íŒŒì¼ì´ ìë™ìœ¼ë¡œ ë²¡í„°í™”ë˜ì–´ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.")

query = st.text_input("â“ ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”")
search_mode = st.radio("ê²€ìƒ‰ ëª¨ë“œ", ["ì¼ë°˜ ê²€ìƒ‰", "ì‹¬ì¸µ ë¦¬ì„œì¹˜"], horizontal=True)

if query:
    if os.path.exists(DB_PATH):
        db = FAISS.load_local(DB_PATH, embedding)
        docs = db.similarity_search(query, k=5)
        doc_context = "\n\n".join([doc.page_content for doc in docs])
    else:
        doc_context = "(ë¬¸ì„œ ì—†ìŒ)"

    web_results = search_tool.run(query)

    if search_mode == "ì‹¬ì¸µ ë¦¬ì„œì¹˜":
        prompt = f"""
[ë¬¸ì„œ ê¸°ë°˜ ì •ë³´]
{doc_context}

[ì›¹ ê²€ìƒ‰ ì •ë³´]
{web_results}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{query}'ì— ëŒ€í•´ ë‹¤ìŒ í•­ëª©ì„ í¬í•¨í•œ ì‹¬ì¸µ ë¶„ì„ ë³´ê³ ì„œë¥¼ ì‘ì„±í•´ì£¼ì„¸ìš”:
1. í•µì‹¬ ìš”ì•½
2. ì£¼ìš” ê·¼ê±° ë° ë°°ê²½ ì •ë³´
3. ì „ëµì  ì‹œì‚¬ì  ë° ì œì–¸
"""
    else:
        prompt = f"""
[ë¬¸ì„œ ê¸°ë°˜ ì •ë³´]
{doc_context}

[ì›¹ ê²€ìƒ‰ ì •ë³´]
{web_results}

ìœ„ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ '{query}'ì— ë‹µë³€í•´ ì£¼ì„¸ìš”.
"""

    response = llm.invoke(prompt)
    st.markdown("### ğŸ’¬ GPT ì‘ë‹µ")
    st.write(response.content)
